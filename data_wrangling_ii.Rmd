---
title: "Data Wrangling II"
author: "Kristal Quispe"
date: "10/19/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(httr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Get some data

read in the NSDUH data

```{r}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"

drug_use_xml = read_html(url)

#You can also copy and paste the url into the read_html. Tables usually works to get tables from websites, not alwasy though. 

table_list = drug_use_xml %>% html_nodes(css ="table")


(table_list) %>% .[[1]]
#Here the (...) is being pipeted into the . (period) and of that we are pulling out a single html node (the first table) from a list of 15 objects. Dot(.) is a place holder for the list of 15 objects. 


tabl_marj = 
  table_list[[1]] %>% 
  html_table() %>% 
  slice(-1) %>%  #Caption at the bottom of the table has been assigned as the first row, repeating over and over for each columns. So use slice to take out the first row.  
  as_tibble()
#as tibble makes 
```


## get harry potter data

```{r}
hpsaga_html = 
  read_html("https://www.imdb.com/list/ls000630791/")
```


```{r}
hp_movie_names =
  hpsaga_html %>% 
  html_nodes(".lister-item-header a") %>% 
  html_text() #converts html to text. Now you have a vector with just the titles. 
hp_movie_runtime = 
  hpsaga_html %>% 
  html_nodes(".runtime") %>% 
  html_text()

hp_movie_money = 
  hpsaga_html %>% 
  html_nodes(".text-small:nth-child(7) span:nth-child(5)") %>% 
  html_text()

hp_df = 
  tibble(
    title = hp_movie_names,
    runtime = hp_movie_runtime,
    money = hp_movie_money
  )
```

## get napoleon

```{r}
url = "https://www.amazon.com/product-reviews/B00005JNBQ/ref=cm_cr_arp_d_viewopt_rvwer?ie=UTF8&reviewerType=avp_only_reviews&sortBy=recent&pageNumber=2"

dynamite_html = read_html(url)

review_titles = 
  dynamite_html %>%
  html_nodes(".a-text-bold span") %>%
  html_text()

review_stars = 
  dynamite_html %>%
  html_nodes("#cm_cr-review_list .review-rating") %>%
  html_text()
#this includes the css selector for ratings on amazon

review_text = 
  dynamite_html %>%
  html_nodes(".review-text-content span") %>%
  html_text()

reviews = tibble(
  title = review_titles,
  stars = review_stars,
  text = review_text
)
```


## Use API

```{r}
nyc_water_df = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>% 
  content()
#GET is one of the four API cores, that calls the data. content pulls the data and parses it assuming csv. 
```

```{r}
nyc_water_df_json = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.json") %>% 
  content("text") %>%
  jsonlite::fromJSON() %>%
  as_tibble()

#the json format is messier. Its more flexible way of exposirintg data, but requires a bit mroe parsing.  In comparison to the csv file. This looks like a data rectangle. 
```


```{r}
brfss_data = 
  GET("https://data.cdc.gov/api/views/waxm-p5qv/rows.csv?accessType=DOWNLOAD") %>% 
  content("parsed")

#using an api is the most reproducible way of having the most up to date dataset
```


```{r}
poke = 
  GET("http://pokeapi.co/api/v2/pokemon/1") %>%
  content()

poke$name
poke$abilities
```